{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28a44942-e412-40cc-b531-0f7fbb6b6323",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53071e0e-765b-4e01-b6e3-e142face6ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class AModel(nn.Module):\n",
    "#     def __init__(self, lr, out_dim):\n",
    "#         super(AModel, self).__init__()\n",
    "\n",
    "#         self.conv1 = nn.Conv2d(1,32,kernel_size = 2,stride=1, padding=0)\n",
    "#         self.conv2 = nn.Conv2d(32,64,kernel_size = 2,stride=2, padding=0)\n",
    "#         self.conv3 = nn.Conv2d(64,64,kernel_size = 2,stride=2, padding=0)\n",
    "        \n",
    "#         self.B = nn.Conv2d(32,32,kernel_size = 1,stride=1, padding=0)\n",
    "#         self.C = nn.Conv2d(32,32,kernel_size = 1,stride=1, padding=0)\n",
    "#         self.D = nn.Conv2d(32,32,kernel_size = 1,stride=1, padding=0)\n",
    "        \n",
    "#         self.fc1 = nn.Linear(2304, 512)\n",
    "#         self.out = nn.Linear(512, out_dim)\n",
    "\n",
    "#     def forward(self,state):\n",
    "#         x = F.relu(self.conv1(state))\n",
    "        \n",
    "#         chA   = self.chanelAtention(x)\n",
    "#         poseA = self.poseAtention(x)\n",
    "        \n",
    "#         x = T.add(chA, x)\n",
    "#         x = T.add(poseA, x)\n",
    "        \n",
    "#         x = F.relu(self.conv2(x))\n",
    "#         x = F.relu(self.conv3(x))\n",
    "        \n",
    "#         x = T.flatten(x, 1)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.out(x)\n",
    "#         output = F.log_softmax(x, dim=1)\n",
    "#         return output\n",
    "    \n",
    "#     def chanelAtention(self, x):\n",
    "        \n",
    "#         batch, ch, width, height = x.shape\n",
    "                \n",
    "#         b =  x.reshape(batch, ch, width*height) # to [chanells, w*h]\n",
    "#         c =  x.reshape(batch, ch, width*height).mT # to [chanells, w*h].T\n",
    "#         d =  x.reshape(batch, ch, width*height)\n",
    "\n",
    "#         M_d = T.matmul(b,c)\n",
    "#         S = T.softmax(M_d, dim=1)\n",
    "#         out = T.matmul(S.mT,d).reshape(batch,ch, width, height)\n",
    "\n",
    "#         return out\n",
    "\n",
    "#     def poseAtention(self, x):\n",
    "        \n",
    "#         b = self.B(x)\n",
    "#         c = self.C(x)\n",
    "#         d = self.D(x)\n",
    "        \n",
    "#         batch, ch, width, height = x.shape\n",
    "#         b =  b.reshape(batch, ch, width*height) # to [chanells, w*h]\n",
    "#         c =  c.reshape(batch, ch, width*height).mT # to [chanells, w*h].T\n",
    "#         d =  d.reshape(batch, ch, width*height)\n",
    "\n",
    "#         M_d = T.bmm( c,b)\n",
    "#         S = T.softmax(M_d, dim=1)\n",
    "#         out = T.matmul(d,S.mT).reshape(batch,ch, width, height)\n",
    "\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae8b629f-21a1-46fa-9f79-f54bbbbb46bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class AModel(nn.Module):\n",
    "#     def __init__(self, lr, out_dim):\n",
    "#         super(AModel, self).__init__()\n",
    "\n",
    "#         self.conv1 = nn.Conv2d(1,32,kernel_size = 2,stride=1, padding=0)\n",
    "#         self.conv2 = nn.Conv2d(32,64,kernel_size = 2,stride=2, padding=0)\n",
    "#         self.conv3 = nn.Conv2d(64,64,kernel_size = 2,stride=2, padding=0)\n",
    "        \n",
    "#         self.B = nn.Conv2d(32,32,kernel_size = 1,stride=1, padding=0)\n",
    "#         self.C = nn.Conv2d(32,32,kernel_size = 1,stride=1, padding=0)\n",
    "#         self.D = nn.Conv2d(32,32,kernel_size = 1,stride=1, padding=0)\n",
    "        \n",
    "#         self.fc1 = nn.Linear(2304, 512)\n",
    "#         self.out = nn.Linear(512, out_dim)\n",
    "\n",
    "#     def forward(self,state):\n",
    "#         x = F.relu(self.conv1(state))\n",
    "        \n",
    "#         chA   = self.chanelAtention(x)\n",
    "#         poseA = self.poseAtention(x)\n",
    "        \n",
    "#         x = T.add(chA, x)\n",
    "#         x = T.add(poseA, x)\n",
    "        \n",
    "#         x = F.relu(self.conv2(x))\n",
    "#         x = F.relu(self.conv3(x))\n",
    "        \n",
    "#         x = T.flatten(x, 1)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.out(x)\n",
    "#         output = F.log_softmax(x, dim=1)\n",
    "#         return output\n",
    "    \n",
    "#     def chanelAtention(self, x):\n",
    "        \n",
    "#         batch, ch, width, height = x.shape\n",
    "                \n",
    "#         b =  x.reshape(batch, ch, width*height) # to [chanells, w*h]\n",
    "#         c =  x.reshape(batch, ch, width*height).mT # to [chanells, w*h].T\n",
    "#         d =  x.reshape(batch, ch, width*height)\n",
    "\n",
    "#         M_d = T.matmul(b,c)\n",
    "#         S = T.softmax(M_d, dim=1)\n",
    "#         out = T.matmul(S.mT,d).reshape(batch,ch, width, height)\n",
    "\n",
    "#         return out\n",
    "\n",
    "#     def poseAtention(self, x):\n",
    "        \n",
    "#         b = self.B(x)\n",
    "#         c = self.C(x)\n",
    "#         d = self.D(x)\n",
    "        \n",
    "#         batch, ch, width, height = x.shape\n",
    "#         b =  b.reshape(batch, ch, width*height) # to [chanells, w*h]\n",
    "#         c =  c.reshape(batch, ch, width*height).mT # to [chanells, w*h].T\n",
    "#         d =  d.reshape(batch, ch, width*height)\n",
    "\n",
    "#         M_d = T.bmm( c,b)\n",
    "#         S = T.softmax(M_d, dim=1)\n",
    "#         out = T.matmul(d,S.mT).reshape(batch,ch, width, height)\n",
    "\n",
    "#         return out\n",
    "    \n",
    "\n",
    "\n",
    "# class AModel(nn.Module):\n",
    "#     def __init__(self, lr, out_dim):\n",
    "#         super(AModel, self).__init__()\n",
    "\n",
    "#         self.conv1 = nn.Conv2d(1,32,kernel_size = 2,stride=1, padding=0)\n",
    "#         self.conv2 = nn.Conv2d(32,64,kernel_size = 2,stride=2, padding=0)\n",
    "#         self.conv3 = nn.Conv2d(64,64,kernel_size = 2,stride=2, padding=0)\n",
    "        \n",
    "#         self.V = nn.Conv2d(32,32,kernel_size = 1,stride=1, padding=0, bias=False)\n",
    "#         self.Q = nn.Conv2d(32,32,kernel_size = 1,stride=1, padding=0, bias=False)\n",
    "#         self.K = nn.Conv2d(32,32,kernel_size = 1,stride=1, padding=0, bias=False)\n",
    "        \n",
    "#         self.fc1 = nn.Linear(2304, 512)\n",
    "#         self.out = nn.Linear(512, out_dim)\n",
    "\n",
    "#     def forward(self,state):\n",
    "#         x = F.relu(self.conv1(state))\n",
    "\n",
    "#         q = self.Q(x).permute(0,1,3,2)\n",
    "#         k = self.K(x)\n",
    "#         v = self.V(x)\n",
    "        \n",
    "#         transpose_dot = T.matmul(k, q)\n",
    "#         S = T.softmax(transpose_dot, dim = 2)\n",
    "#         atention = T.matmul(S, v) \n",
    "#         x = T.add(atention, x)\n",
    "        \n",
    "#         x = F.relu(self.conv2(x))\n",
    "#         x = F.relu(self.conv3(x))\n",
    "        \n",
    "#         x = T.flatten(x, 1)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.out(x)\n",
    "#         output = F.log_softmax(x, dim=1)\n",
    "#         return output\n",
    "\n",
    "\n",
    "class AModel(nn.Module):\n",
    "    def __init__(self, lr, out_dim):\n",
    "        super(AModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = T.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "378dde41-80a8-49bc-b121-38a4a60bc90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = AModel(lr = 0.001, out_dim = 5).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37bff350-df3b-4e6d-b04a-7c26f73dd4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = T.rand((7,1,30,30)).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2deb46c7-9054-49fe-8893-e4a500f5f632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aec7fe8-d0e6-4a55-aee3-039469aede82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2032ac5f-259f-412a-9c5e-70fc3348d442",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train( model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # if batch_idx % 10 == 0:\n",
    "        #     # print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "        #     #     epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "        #     #     100. * batch_idx / len(train_loader), loss.item()))\n",
    "        #     pass\n",
    "        #     if False:\n",
    "        #         break\n",
    "    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30fc66be-ee38-4452-9809-f5b292884749",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with T.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    return test_loss , 100. * correct / len(test_loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36acd3af-02d8-4aa4-b952-181dbaf8b00d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 2.0534, Accuracy: 7293/10000 (73%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 1.7078, Accuracy: 7796/10000 (78%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 1.4109, Accuracy: 7959/10000 (80%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 1.2152, Accuracy: 8075/10000 (81%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 1.0949, Accuracy: 8166/10000 (82%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 1.0201, Accuracy: 8236/10000 (82%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.9725, Accuracy: 8292/10000 (83%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.9415, Accuracy: 8312/10000 (83%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.9208, Accuracy: 8331/10000 (83%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.9069, Accuracy: 8339/10000 (83%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.8975, Accuracy: 8347/10000 (83%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.8910, Accuracy: 8354/10000 (84%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.8865, Accuracy: 8358/10000 (84%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.8834, Accuracy: 8359/10000 (84%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.8813, Accuracy: 8362/10000 (84%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "T.manual_seed(3)\n",
    "\n",
    "device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_kwargs = {'batch_size': 128}\n",
    "test_kwargs = {'batch_size': 128}\n",
    "if T.cuda.is_available():\n",
    "    cuda_kwargs = {'num_workers': 1,\n",
    "                   'pin_memory': True,\n",
    "                   'shuffle': True}\n",
    "    train_kwargs.update(cuda_kwargs)\n",
    "    test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transform)\n",
    "dataset2 = datasets.MNIST('../data', train=False,\n",
    "                   transform=transform)\n",
    "train_loader = T.utils.data.DataLoader(dataset1,**train_kwargs)\n",
    "test_loader = T.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "\n",
    "model = AModel(lr = 0.001, out_dim=10).to(device)\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=0.001)\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n",
    "for epoch in range(1, 15 + 1):\n",
    "    train( model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)\n",
    "    scheduler.step()\n",
    "\n",
    "# if args.save_model:\n",
    "#     T.save(model.state_dict(), \"mnist_cnn.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d09215-47a3-4842-90c0-5047c0597694",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68aab7d-a4d8-42eb-b85c-80dcb7d7ea07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9648a2d-7f82-428a-b48b-e28d800828a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b24be1-2b00-4f2a-b7b2-786c8c9e9124",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e992228f-c379-4b49-af11-43f4d5df20b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccff14f-800a-4bcc-bd37-dfbfa7af31b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8caff0-dadc-4b2f-872d-eb40d906eb14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c06c63b-9735-48c9-842e-a7b842c8dc55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25f36e3b-02b0-4ae2-bbdd-756613957198",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = T.rand((1,4,3))\n",
    "n = m.permute(2,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9de24e46-c5b7-4441-9021-5a6b223613a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = T.matmul(n,m)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af2ce097-6690-4ec3-8c1c-941f58d8a81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "S = T.softmax(out, dim = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32e6ee12-f012-4cdc-8161-aef752f0aad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4675, 0.5477, 0.3171, 0.5302]],\n",
       "\n",
       "        [[0.4466, 0.5304, 0.3022, 0.4804]],\n",
       "\n",
       "        [[0.4456, 0.5137, 0.3500, 0.5019]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " T.matmul(S, m.permute(0,2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b75f3fae-fa6b-43a4-8c65-fc7ad348d956",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = T.rand((10,2,4,5))\n",
    "batch,ch, width, height = m.shape\n",
    "b =  m.reshape(batch, ch, width*height) # to [chanells, w*h]\n",
    "c =  m.reshape(batch, ch, width*height).mT # to [chanells, w*h].T\n",
    "d =  m.reshape(batch, ch, width*height)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07a4dfad-3776-4ddd-af05-d63a149eb226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 2, 4, 5])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# T.matmul(b,c).shape\n",
    "M_d = T.bmm( c,b)\n",
    "# M_d.shape\n",
    "S = T.softmax(M_d, dim=1)\n",
    "# S.shape, d.shape\n",
    "out = T.matmul(d,S.mT).reshape(batch,ch, width, height)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bdd2ed-724e-4734-8047-c4649314189b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95542f62-a263-4765-b2e8-862e364533b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112ad1e1-dab6-4515-98b3-46d837a65b52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a070999-9edd-4652-965b-d275f6055615",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43ff18f5-d873-400a-9ff2-6d17297073ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "# position Atention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e81720b-248b-407f-9356-0e89f6dd28e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 2, 4, 5])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = T.rand((10,2,4,5))\n",
    "batch,ch, width, height = m.shape\n",
    "b =  m.reshape(batch, ch, width*height) # to [chanells, w*h]\n",
    "c =  m.reshape(batch, ch, width*height).mT # to [chanells, w*h].T\n",
    "d =  m.reshape(batch, ch, width*height)\n",
    "\n",
    "M_d = T.bmm( c,b)\n",
    "S = T.softmax(M_d, dim=1)\n",
    "out = T.matmul(d,S.mT).reshape(batch,ch, width, height)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb5376b-a797-4616-9c4c-254fb0dbab19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76516a5d-5840-46b8-92ad-567963bffc07",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Chanell Atention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "660cf86d-10c2-4330-8cdf-02ba5cc5dcea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 2, 4, 5])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = T.rand((10,2,4,5))\n",
    "batch,ch, width, height = m.shape\n",
    "\n",
    "b =  m.reshape(batch, ch, width*height) # to [chanells, w*h]\n",
    "c =  m.reshape(batch, ch, width*height).mT # to [chanells, w*h].T\n",
    "d =  m.reshape(batch, ch, width*height)\n",
    "\n",
    "M_d = T.matmul(b,c)\n",
    "# M_d.shape\n",
    "S = T.softmax(M_d, dim=1)\n",
    "# print(S.shape)\n",
    "out = T.matmul(S.mT,d).reshape(batch,ch, width, height)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82ed687-44e4-4d1e-8d08-50b8bc8186c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1090e6-0444-4460-b030-d6de5b1be1f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e11b84e-a570-495c-bafd-82fb8f32901a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3451d843-6df2-4ad1-85de-3bd26c73af69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683984e8-ecf7-45ef-8f03-9b7e1b5ac673",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cbfcaa-e140-44dc-b05a-6cdb58d0f089",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c3477e-fee8-4153-9d25-9bd381d1bb1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc883fb-d74b-428a-abe6-af581e0ee932",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0c2ede-9209-4749-a72d-e337de14809e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cc6b34-9ef5-4d72-bec2-7c354fe15613",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b0c277-dd39-4284-b7b6-10439882daeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
